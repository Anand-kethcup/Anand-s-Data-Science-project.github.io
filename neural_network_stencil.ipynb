{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe3XhWAyJzTg"
      },
      "source": [
        "# CS 410/1411 Homework 7: Logistic Regression and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA7b2JqRKDCC"
      },
      "source": [
        "Welcome to Homework 7!\n",
        "\n",
        "In this assignment, you'll be building multiple binary classifiers. The first two will use logistic regression, and the second, perceptrons. You will be applying these classifiers to classify points in a pickaxe drawing, where the points on the handle are one color, and the axes, another. As such points are not linearly separable, the logistic regression classifier will not succeed. Neither will a simple perceptron. A multilayer perceptron, however, will accomplish this task.\n",
        "\n",
        "What you will know:\n",
        "- what it means for data to be linearly separable\n",
        "- how backpropagation works\n",
        "\n",
        "What you will be able to do:\n",
        "- build a binary classifier using logistic regression\n",
        "- build a binary classifier using perceptrons\n",
        "- use Python's `PyTorch` library\n",
        "\n",
        "Let's *dig* into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTl9eQWukJQW"
      },
      "source": [
        "## Binary Classification Task\n",
        "\n",
        "In this assignment, you will train a binary classifier to predict the color (brown or blue) of a point $(x_1, x_2)$ on the Cartesian plane. In fact, your classifier will not predict merely 0 or 1; rather, it will predict a probability $p \\in [0, 1]$ that the point is blue.\n",
        "\n",
        "**Checkpoint**: What would a prediction of $0.9$ signify? And what about $0.05$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI0zSdHCKbvD"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic regression is used for binary classification, i.e., to classify a point as either 1 or 0. In fact, points are assigned probabilities that their classification is 1, much like you might do at an impure leaf of a decision tree.\n",
        "\n",
        "Assume we are given a data set $\\mathcal{D} = \\{ (\\bm{x}_i, y_i) \\}_{i=1}^m$ of $m$ sample points $\\bm{x}_i$ with corresponding labels $y_i$.\n",
        "\n",
        "As linear models are the simplest to build, the main idea of logistic regression is to fit a linear model, with parameters, say,  $\\bm{w}$ and $b$, not to the $(\\bm{x}_i, y_i)$ directly as in linear regression, but rather to the odds ratio: i.e., the ratio of the probability of class 1 to the probability of class 0. Given a point $\\bm{x}$,\n",
        "$$ \\frac{p (\\bm{x})}{1 - p (\\bm{x})} = \\bm{w}^T \\bm{x} + b $$\n",
        "\n",
        "In fact, for mathematical simplicity, a linear model is fit to the natural log of the odds ratio instead:\n",
        "$$ \\ln \\left( \\frac{p (\\bm{x})}{1 - p (\\bm{x})} \\right) = \\bm{w}^T \\bm{x} + b $$\n",
        "\n",
        "Rearranging this formula yields:\n",
        "$$ p (\\bm{x}) = \\frac{1}{1 + e^{- \\left( \\bm{w}^T \\bm{x} + b \\right) }} $$\n",
        "\n",
        "Our goal is to infer, or learn, parameter values for $\\bm{w}$ and $b$ so that $p (\\bm{x})$ accurately reflects the class to which $\\bm{x}$ belongs: i.e., $p (\\bm{x})$ is close to 1, when $\\bm{x}$ is labeled 1; and $p (\\bm{x})$ is close to 0, otherwise.\n",
        "\n",
        "We call the process of using our model to predict a label $\\hat{y}$ from $\\bm{x}$ **forward propagation** or the **forward pass**.\n",
        "\n",
        "Let $p_i \\doteq p (\\bm{x}_i)$ denote the model's prediction on input $\\bm{x}_i$ with true label $y_i \\in \\{ 0, 1 \\}$. The model's loss on this input can be described as $$- (y_i \\log p_i + (1 - y_i) \\log (1 - p_i))$$ a quantity known as **cross entropy**. When $y_i = 1$, this loss evaluates to $- \\log p_i$; and when $y_i = 0$, this loss evaluates to $- \\log (1 - p_i)$. In this way, if $y_i = 1$, values of $p_i$ close to 1 yield loss close to 0; on the other hand, if $y_i = 0$, values of $p_i$ close to 0 yield loss close to 0.\n",
        "\n",
        "<!-- **Cross Entropy:** Given two Bernoulli random variables $A$ and $B$, the expression $- \\left( A \\log B + (1 - A) \\log (1 - B) \\right)$ is known as the **cross entropy** between them. -->\n",
        "\n",
        "**Checkpoint**: The loss contains two terms. What do these terms represent? What happens to the loss when $p_i \\approx y_i$? And what happens when they differ dramatically?\n",
        "\n",
        "The goal in logistic regression is find $\\bm{p} = (p_1, \\ldots, p_m)$ that minimize the cross entropy of the data:\n",
        "$$\n",
        "C_{\\mathcal{D}} (\\bm{p}) \\doteq - \\sum_{i = 1}^m (y_i \\log p_i + (1 - y_i) \\log (1 - p_i))\n",
        "$$\n",
        "This optimization is over the parameters $\\bm{w}$ and $b$ that define each $p_i$ as a function of $\\bm{x}_i$, so we will express it as $C (\\bm{w}, b)$, dropping the subscript ${\\mathcal{D}}$, as it is clear from context.\n",
        "\n",
        "As it turns out, this optimization problem does not have a closed-form solution. Instead, we are forced to resort to gradient descent. The partial derivatives of $C$ with respect to each of its parameters is given by:\n",
        "$$\\frac{\\partial C (\\bm{w}, b)} {\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (p(\\bm{x}_i) - y_i) x_{ij}$$\n",
        "and\n",
        "$$\\frac{\\partial C (\\bm{w}, b)} {\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (p(\\bm{x}_i) - y_i)$$\n",
        "\n",
        "We update the parameters of the model $\\bm{w}$ and $b$ using this gradient information as follows:\n",
        "$$\\bm{w}^{\\textrm{new}}_j = \\bm{w}^{\\textrm{old}}_j - \\alpha \\frac{\\partial C (\\bm{w}, b)}{\\partial \\bm{w}_j}$$\n",
        "and\n",
        "$$b^{\\textrm{new}} = b^{\\textrm{old}} - \\alpha \\frac{\\partial C (\\bm{w}, b)}{\\partial b}$$\n",
        "\n",
        "where $\\alpha$ is a learning rate.\n",
        "\n",
        "This process of calculating gradients and updating the model's parameters is called the **backward pass** or **backpropagation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Hcxa4b2Wd_"
      },
      "source": [
        "<!-- ASIDE: MLE Interpretation of Logistic Regression\n",
        "\n",
        "We model each class label as a draw from a Bernoulli random variable $Y_i$ which takes value $1$ with probability $p_i \\doteq p (\\bm{x}_i)$, and value 0, otherwise:\n",
        "$$\n",
        "\\mathcal{P}_{p_{i}} (Y_i) =\n",
        "\\begin{cases}\n",
        "p_i & \\text{if $Y_i = 1$} \\\\\n",
        "1 - p_i & \\text{if $Y_i = 0$}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Equivalently, $\\mathcal{P}_{p_{i}} (Y_i) = p_i^{Y_i} (1 - p_i)^{1 - {Y_i}}$.\n",
        "\n",
        "We can reinterpret this family of probability functions as a likelihood function: $\\mathcal{L}_{y_i} (p_i) = p_i^{Y_i} (1 - p_i)^{1 - {Y_i}}$, given data point $(\\bm{x}_i, y_i)$. Taking logs yields\n",
        "$$\\log \\mathcal{L}_{y_i} (p_i) = Y_i \\log p_i + (1 - Y_i) \\log (1 - p_i)$$\n",
        "\n",
        "Recall that $p_i$ is defined by the parameters $\\bm{w}$ and $b$. The goal in logistic regression, therefore, is to find parameters $\\bm{w}$ and $b$ that maximize the (log) likelihood of the data, as defined by this expression.\n",
        "\n",
        "**Note:** The negative log likelihood $\\log \\mathcal{L}_{y_i} (p_i)$ is an expression of the cross entropy between Bernoulli random variable $Y_i$, the label, and the Bernoulli random variable that describes the model's predicted labels, i.e., 1 with probability $p_i$. Logistic regression is also often described as minimizing cross entropy \"loss\", rather than as maximizing likelihood. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbGEqpBlkTI"
      },
      "source": [
        "## Part 0: The Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTavQ1XJ2WeA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7jiYKcNE3t45"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "dToPNx6FrxfD",
        "outputId": "eacafc49-b647-404f-c8fe-1f25540acb35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/NoisyPickaxe.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6731163eba86>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdata_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NoisyPickaxe.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6731163eba86>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Generate the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/NoisyPickaxe.png'"
          ]
        }
      ],
      "source": [
        "BLUE = '#2bc7ad'\n",
        "BLUE_PIXEL = np.array([104, 78, 30])\n",
        "BROWN = '#684e1e'\n",
        "BROWN_PIXEL = np.array([43, 199, 172])\n",
        "\n",
        "def load_data(filename: str):\n",
        "    \"\"\"\n",
        "    Load image of pickaxe (or other image with the filename)\n",
        "    Load n pixels of pickaxe and returns array of shape (m, 3)\n",
        "    For each pixel, store the x, y, and label\n",
        "\n",
        "    plot the data as well.\n",
        "    Input:\n",
        "        filename: file where data is stored\n",
        "    Output:\n",
        "        data_points: numpy array of shape (m, 3).\n",
        "    \"\"\"\n",
        "    # Generate the dataset\n",
        "    image = Image.open(filename)\n",
        "    width, height = image.size\n",
        "    pixels = list(image.getdata())\n",
        "\n",
        "    # pixels holds RGB values for all pixels in image\n",
        "    pixels = np.array([pixels[i * width:(i + 1) * width] for i in range(height)])\n",
        "\n",
        "    data_points_blue = []\n",
        "    data_points_brown = []\n",
        "\n",
        "    data_points = []\n",
        "\n",
        "    for i in reversed(range(height)):\n",
        "        for j in range(width):\n",
        "            pixel = pixels[height-i-1, j, :]\n",
        "            if np.array_equal(pixel, BROWN_PIXEL):\n",
        "                data_points.append(np.array([j, i, 1]))\n",
        "                data_points_brown.append([j, i])\n",
        "            elif np.array_equal(pixel, BLUE_PIXEL):\n",
        "                data_points.append(np.array([j, i, 0]))\n",
        "                data_points_blue.append([j, i])\n",
        "\n",
        "    data_points = np.array(data_points)\n",
        "    data_points_blue = np.array(data_points_blue)\n",
        "    data_points_brown = np.array(data_points_brown)\n",
        "\n",
        "    # Visualize the dataset\n",
        "    plt.scatter(data_points_blue[:, 0], data_points_blue[:, 1], c=BLUE, s=5) # blue-ish\n",
        "    plt.scatter(data_points_brown[:, 0], data_points_brown[:, 1], c=BROWN, s=5) # Brown\n",
        "\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return data_points\n",
        "\n",
        "data_points = load_data(\"NoisyPickaxe.png\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAAc5NEB2WeB"
      },
      "source": [
        "Observe that this dataset is *not* linearly separable, there is no line we can draw on this image that perfectly separates the blue and brown pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZq6tijl2WeB"
      },
      "outputs": [],
      "source": [
        "def create_dataset(data_points: np.ndarray):\n",
        "  \"\"\"\n",
        "  Take in data_points of shape (m, 3), return X, y features and labels\n",
        "\n",
        "  Inputs:\n",
        "    - data_points: (m, 3) array of x, y, label\n",
        "  Outputs:\n",
        "    - X: Feature Matrix of (m, 2)\n",
        "    - y: Label vector of shape (m, 1)\n",
        "  \"\"\"\n",
        "  # Gather X, y for dataset\n",
        "  X = np.zeros((data_points.shape[0], 2)) # All of our points\n",
        "  for idx, pixel in enumerate(data_points):\n",
        "    X[idx, :] = pixel[0:2]\n",
        "\n",
        "  # The corresponding labels for our points\n",
        "  y = data_points[:, 2]\n",
        "  y = y.reshape(-1, 1)\n",
        "\n",
        "  X = X / (X.max() - X.min())\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X, y = create_dataset(data_points)\n",
        "print(\"Features shape:\", X.shape, \"Labels shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpnrOg8WlpJZ"
      },
      "source": [
        "## Part 1: Logistic Regression with NumPy and Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plah66302Zzf"
      },
      "source": [
        "In this section, you'll implement logistic regression (from scratch) using `NumPy`.\n",
        "\n",
        "To do so, you will code the following functions:\n",
        "1. `initialize_parameters`: Initialize the weights and bias terms to 0\n",
        "1. `sigmoid`: Implement the sigmoid activation function using numpy\n",
        "1. `forward`: Run the model's forward pass, i.e., compute the model's predicted probabilities\n",
        "1. `backward`: Computes the gradients of the loss with respect to the model's parameters\n",
        "1. `predict`: Converts the model's predicted probabilities into binary labels (1 iff the prediction exceeds 0.5)\n",
        "1. `accuracy`: Returns the model's accuracy (i.e., its average number of mistakes)\n",
        "1. `optimize`: Use gradient descent to minimize cross entropy loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVH3J8MV2WeB"
      },
      "source": [
        "### Task: Helper Functions\n",
        "\n",
        "You should start by implementing the requisite helper functions.\n",
        "\n",
        "Notes about numpy:\n",
        "1. \"shape\" errors are errors that arise when you try to run an operation with incompatible matrices. For example, you cannot multiply two matrices with shapes (1, 2), (3, 4). Debugging shape errors comes down to tracking the shapes of your arrays leading up to the error. It is important from both a programming and conceptual point of view that you understand what the shapes of your matrices *should* be. If you ask a TA a question about a shape error, the first thing they'll ask you is what the shapes *should* be.\n",
        "1. None of these functions require a for loop. We will deduct points if a for loop is used in these helper functions. For instance, you should not write a loop to sum the elements of a vector, when there is a numpy function that does exactly that (i.e., `np.sum`).\n",
        "1. One very useful numpy operation is to create a *boolean array*. For example, consider the numpy array `a=np.array((1, 2, 3, 4, 5))`. The operation `a % 2 == 0` creates the array of booleans `np.array([False, True, False, True, False])`. If you wanted to count the number of even numbers in `a`, you could simply run `np.sum(a % 2 == 0)` (Python considers False=0 and True=1).\n",
        "1. If you find you are missing a dimension, i.e., you want a array of shape $(n, 1)$ and have an array of shape $(n,)$, you can use the `reshape` method to add the additional dimension. Reshape takes a desired shape as input. Again, if you do not know what the shape should be you will run into many issues trying to debug your code. You must understand the shapes of your vectors and matrices in order to work efficiently. Trial and error won't work for numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHMAU9mP2WeB"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(d: int):\n",
        "    \"\"\"\n",
        "    Initialize w vector and bias term based on the number of features\n",
        "    Hint: you can use np.zeros to initialize a vector of all 0s with a given shape.\n",
        "    In practice, you may want to use random initial parameters, but all 0s works fine here.\n",
        "\n",
        "    You should confirm the shapes of your parameters are correct before doing anything else!\n",
        "    If these are wrong, everything down the line will be wrong as well.\n",
        "\n",
        "    Inputs:\n",
        "        - d: number of features in dataset\n",
        "    Outputs:\n",
        "        - w: weights vector of shape (d, 1)\n",
        "        - b: bias term (float)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    w = np.zeros((d, 1))\n",
        "    b = 0.0\n",
        "    return w, b\n",
        "\n",
        "def sigmoid(z: np.ndarray):\n",
        "    \"\"\"\n",
        "    Implement the sigmoid function.\n",
        "\n",
        "    sigmoid(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "    Must work where z is a single input (i.e., a float) or a vector (i.e., an array of shape (m))\n",
        "\n",
        "    Inputs:\n",
        "        - z: input value\n",
        "    Outputs:\n",
        "        - output of sigmoid(z)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "def forward(w, b, X):\n",
        "    \"\"\"\n",
        "    Run a forward pass of logistic regression given model parameters (w, b) and data X\n",
        "    Compute the output of the logistic regression from input matrix X and parameters w and b\n",
        "\n",
        "    Inputs:\n",
        "        - w: weights term of shape (d, 1)\n",
        "        - b: bias term (float)\n",
        "        - X: input data of shape (m, d)\n",
        "    Output:\n",
        "        - predicted probabilities of shape (m, 1)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return sigmoid(np.dot(X,w) + b)\n",
        "\n",
        "def backward(p_pred, X, y):\n",
        "    \"\"\"\n",
        "    Given the model's inputs (X), outputs (p_pred), and true labels (y), compute the gradients of the loss with respect to w and b\n",
        "    Return a tuple of (dw, db), where dw is the gradient wrt w and db is the gradient wrt b.\n",
        "    dw should be a column vector of shape (d, 1) and db should be a scalar.\n",
        "\n",
        "    Inputs:\n",
        "        - p_pred: predicted probabilities of shape (m, 1)\n",
        "        - X: input data of shape (m, d)\n",
        "        - y: true labels of shape (m, 1)\n",
        "    Outputs:\n",
        "        - dw: gradient of the loss with respect to w, of shape (d, 1)\n",
        "        - db: gradient of the loss with respect to b, a scalar\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    m = X.shape[0]\n",
        "    dw = np.dot(X.T, (p_pred - y)) / m\n",
        "    db = np.sum(p_pred - y) / m\n",
        "    return dw, db\n",
        "\n",
        "def predict(w, b, X):\n",
        "    \"\"\"\n",
        "    Run logistic regression model forward to compute predictions\n",
        "\n",
        "    Turn predictions into labels\n",
        "    Everything with probability > 0.5 is classified as a 1, everything else as a 0.\n",
        "    Should call forward() and convert the outputs to labels.\n",
        "\n",
        "    This method is used for computing accuracy and plotting decision boundaries.\n",
        "\n",
        "    - Inputs:\n",
        "        - w: weights term of shape (d, 1)\n",
        "        - b: bias term (float)\n",
        "        - X: input data of shape (m, d)\n",
        "    Output:\n",
        "        - predicted labels of shape (m, 1)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    probabilities = forward(w, b, X)\n",
        "    return (probabilities > 0.5).astype(int)\n",
        "\n",
        "def accuracy(predicted_labels, y):\n",
        "    \"\"\"\n",
        "    Compute accuracy between predicted_labels and y\n",
        "\n",
        "    Inputs:\n",
        "        predicted_labels: predicted labels of shape (m, 1)\n",
        "        y: true labels of shape (m, 1)\n",
        "    Outputs:\n",
        "        - accuracy: accuracy of the model (float)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return np.mean(predicted_labels) == y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avcxbdtr2WeC"
      },
      "source": [
        "### Task: Gradient Descent\n",
        "\n",
        "Next, you'll move on to implementing gradient descent.\n",
        "\n",
        "Fill in the `optimize` function so that it first runs the model forward to make predictions, and then backwards to compute the gradients of the loss with respect to the model's parameters. Finally, it updates the parameter values based on their gradients.\n",
        "\n",
        "You should expect your model to exhibit approximately 83% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqnbUFKS2WeC"
      },
      "outputs": [],
      "source": [
        "# Optimization using gradient descent\n",
        "def optimize(w, b, X, y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    For a given set of model parameters (w, b) and dataset (X, y), run gradient descent for num_iterations\n",
        "    Fill in the lines for the forward step (get model outputs), backward step (get gradient dL/dw, dL/db)\n",
        "    and then update parameters w and b accordingly\n",
        "\n",
        "    Inputs:\n",
        "        w: weights term of shape (d, 1)\n",
        "        b: bias (float)\n",
        "        X: input data of shape (m, d)\n",
        "        y: labels of shape (d, 1)\n",
        "        num_iterations: number of iterations to run gradient descent for\n",
        "        learning_rate: The learning rate for parameter updates\n",
        "\n",
        "    Outputs:\n",
        "        params: Model parameters for the final model {\"w\": np.ndarray, \"b\": float}\n",
        "        params_list: List of previous parameter settings (for animation)\n",
        "    \"\"\"\n",
        "    params_list = []\n",
        "    for i in range(num_iterations):\n",
        "        # TODO: Fill in, provide comments but not code\n",
        "        # Forward propagation\n",
        "        p_pred = forward (w, b, X)\n",
        "\n",
        "        # Backward propagation\n",
        "        dw, db = backward(p_pred, X, y)\n",
        "\n",
        "        # Update parameters w, b\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "        # End TODO\n",
        "\n",
        "        # We'll track the parameters throughout the training process for some visualizations.\n",
        "        params_list.append((w, b))\n",
        "\n",
        "    params = {\"w\": w, \"b\": b}\n",
        "\n",
        "    return params, params_list\n",
        "\n",
        "# Extract n_features from X (it's 2 features for x, y of pixels)\n",
        "n_features = X.shape[1]\n",
        "w, b = initialize_parameters(n_features)\n",
        "params_np, params_list = optimize(w, b, X, y, num_iterations=5000, learning_rate=0.001)\n",
        "\n",
        "# Predictions and accuracy\n",
        "predictions_np = predict(params_np[\"w\"], params_np[\"b\"], X)\n",
        "accuracy_np = np.mean(predictions_np == y)\n",
        "print(\"Final Model:\")\n",
        "print(f\"NumPy model accuracy: {accuracy_np:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSvdWTi2WeC"
      },
      "source": [
        "### Task: Decision Boundaries\n",
        "\n",
        "The **decision boundary** of a binary classifier depicts where the decisions change from 0's to 1's. We provide support code so that you can plot the decision boundaries of the binary classifier's you build in this assignment.\n",
        "\n",
        "Our code to generate a decision boundary first samples a large number of inputs, and then applies the model to predict the corresponding outputs. Note that these predictions are made using the `predict` function, not the `forward` function. In particular, they are labels not probabilities.\n",
        "\n",
        "Run the support code below to visualize your logistic regression model's predictions and plot its decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sWvcAkk2WeC"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(w, b, X, y):\n",
        "    \"\"\"\n",
        "    Plot decision boundary of the logistic regression model.\n",
        "    \"\"\"\n",
        "    # Add a little bit of a border to x_min/max and y_min/max for plotting\n",
        "    x_min, x_max = X[:, 0].min() - 0.25, X[:, 0].max() + 0.25\n",
        "    y_min, y_max = X[:, 1].min() - 0.25, X[:, 1].max() + 0.25\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    # Pass all x, y pairs through sigmoid function\n",
        "    all_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = predict(w, b, all_points)\n",
        "\n",
        "    # Convert back to grid-shape\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=1, levels=[-1, 0, 1], colors=[\"#E4C483\", \"#C3FFF6\"])\n",
        "    color_dict = {0 : tuple(BLUE_PIXEL/256), 1 : tuple(BROWN_PIXEL/256)}\n",
        "    colors = [color_dict[label] for label in y[:, 0]]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=5)\n",
        "    pred = predict(w, b, X)\n",
        "    plt.title(f\"NumPy Logistic Regression Decision Boundary\\nAccuracy={accuracy(pred, y):.2f}, Weights=({w[0, 0]:.2f}, {w[1, 0]:.2f}), Bias={b:.2f}\")\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plot = plt.gcf()\n",
        "    return plot\n",
        "\n",
        "plot = plot_decision_boundary(params_np[\"w\"], params_np[\"b\"], X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Y1XlCG2WeC"
      },
      "source": [
        "### Task: Gradient Descent Animation\n",
        "\n",
        "Run the support code below to create an animation of your model's decision boundary--in particular, to see how it changes over the course of running the gradient descent algorithm.\n",
        "\n",
        "**Note:** The rendering process is slow. It may take upwards of two minutes to run. It is very useful to see this animation, but you need not run this cell every time you run through the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4lwz4Mu2WeC"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from matplotlib import animation\n",
        "from matplotlib.animation import PillowWriter\n",
        "import copy\n",
        "\n",
        "def create_gradient_descent_animation(X, y, params_list, filename='gradient_descent.gif', fps=30):\n",
        "    \"\"\"\n",
        "    Create and save an animation of the decision boundary changing during gradient descent.\n",
        "    Note these animations are educational, but can take significant time to run.\n",
        "    \"\"\"\n",
        "    # Set up the figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # add another second of the final frame\n",
        "    # params_list =\n",
        "\n",
        "    def animate(i):\n",
        "        i = min(len(params_list) - 1, i)\n",
        "        w, b = params_list[i]\n",
        "        plot_decision_boundary(w, b, X, y)\n",
        "\n",
        "    # Create the animation\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig,\n",
        "        animate,\n",
        "        frames=len(params_list)+fps,\n",
        "        interval=1000/fps,\n",
        "        blit=False,\n",
        "    )\n",
        "\n",
        "    writer = animation.PillowWriter(\n",
        "        fps=fps,\n",
        "        codec='gif',\n",
        "    )\n",
        "\n",
        "    # Save animation\n",
        "    anim.save(filename, writer=writer)\n",
        "    plt.close()\n",
        "\n",
        "# Create and save the animation, only plot the first 100 steps for speed, you can increase this if desired\n",
        "plot = create_gradient_descent_animation(X, y, params_list[:100], fps=33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB6XBbDHlxJC"
      },
      "source": [
        "## Part 2: Logistic Regression with PyTorch\n",
        "\n",
        "The goal of the next part of the assignment is for you to start to become familiar with the PyTorch library, a popular library for building neural networks. To that end, we've provided you with another implementation of logistic regression, this time using PyTorch. Read through our code carefully, comparing this implementation to your implementation of logistic regression using gradient descent.\n",
        "\n",
        "One similarity you will observe is that `torch` and `numpy` provide a similar set of built-in functions, like `sum`, `mean`, and `log`. But there are also some superficial differences. For instance, torch uses `torch.mm` for matrix multiplication, while `numpy` uses `dot` or `matmul` (although you can use @ for matrix multplication with either library). Additionally, `torch`'s default data type is called a `tensor`, rather than `numpy`'s `ndarray`. A tensor, is a mathematical object similar to matrices. A tensor is a matrix with additional dimensions other than just rows and columns.\n",
        "\n",
        "The key difference between libraries, however, lies in how gradients are computed. In our implementation of logistic regression from scratch, we computed the gradients of the loss function by hand, and coded those formulas directly.\n",
        "\n",
        "In `torch`, in contrast, gradients are computed automatically. We define a loss function, and then call `backward` on this function. After `backward` is called, all `torch` tensors that were used to compute the loss (for which `requires_grad=True`) are automatically endowed with an associated `.grad` attribute, which represents the gradient of the loss (or whatever `backward` was called on) with respect to that tensor.\n",
        "\n",
        "Run our code, and observe that the accuracy and the decision boundary are more or less exactly as they were in your implementation of logistic regression from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am-PabAv2WeC"
      },
      "outputs": [],
      "source": [
        "# Visualize data and model decision boundary\n",
        "def plot_decision_boundary_torch(w, b, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 0.25, X[:, 0].max() + 0.25\n",
        "    y_min, y_max = X[:, 1].min() - 0.25, X[:, 1].max() + 0.25\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.01), torch.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    z = sigmoid_torch(torch.mm(torch.tensor(w.T, dtype=torch.float32), torch.tensor(np.c_[xx.ravel(), yy.ravel()].T, dtype=torch.float32)) + torch.tensor(b, dtype=torch.float32))\n",
        "\n",
        "    z = z > 0.5\n",
        "    z = z.detach().numpy().reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, z, alpha=1, levels=[-1, 0, 1], colors=[\"#E4C483\", \"#C3FFF6\"])\n",
        "    color_dict = {0 : tuple(BLUE_PIXEL/256), 1 : tuple(BROWN_PIXEL/256)}\n",
        "    colors = [color_dict[label] for label in y[:, 0]]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=5)\n",
        "    plt.title(\"PyTorch Logistic Regression Decision Boundary\")\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcRwoE3BlyY2"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "def initialize_parameters(n_features):\n",
        "    \"\"\"\n",
        "    Initialize w, b to torch tensors. w is a (d, 1) column vector,\n",
        "    b is a scalar, but needs to be registered as a tensor for gradients to track.\n",
        "\n",
        "    w and b are specified to require grad to ensure their gradient is computed when backward is called\n",
        "\n",
        "    Inputs:\n",
        "        n_features: number of features\n",
        "    Outputs:\n",
        "        w: Torch Tensor of shape (d, 1) weights\n",
        "        b: Torch Tensor of shape (1) bias term\n",
        "    \"\"\"\n",
        "    w = torch.zeros((n_features, 1), dtype=torch.float32, requires_grad=True)\n",
        "    b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
        "    return w, b\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid_torch(z):\n",
        "    \"\"\"\n",
        "    Torch has a built in sigmoid function. That's pretty convenient, as it's commonly used in Neural Networks\n",
        "\n",
        "    Inputs:\n",
        "        z: torch.tensor of shape (m)\n",
        "    Outputs:\n",
        "        activation: torch.tensor of shape (m)\n",
        "    \"\"\"\n",
        "    return torch.sigmoid(z)\n",
        "\n",
        "# Define cross entropy loss\n",
        "def loss_torch(p_pred, y):\n",
        "    \"\"\"\n",
        "    Log-loss/Binary Cross Entropy loss function\n",
        "    \"\"\"\n",
        "    loss = -torch.mean(y * torch.log(p_pred) + (1 - y) * torch.log(1 - p_pred), dim=0)\n",
        "    return loss\n",
        "\n",
        "# Forward propagation\n",
        "def forward_torch(w, b, X):\n",
        "    z = torch.mm(X, w) + b\n",
        "    A = sigmoid_torch(z)\n",
        "    return A\n",
        "\n",
        "# Optimization using PyTorch's automatic gradient calculation\n",
        "def optimize_pytorch(w, b, X, y, num_iterations, learning_rate, track_gradients=False):\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Forward propagation within a torch gradient tracker\n",
        "        p_pred = forward_torch(w, b, X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_torch(p_pred, y)\n",
        "\n",
        "        # Call backward on loss to run the backpropagation step\n",
        "        loss.backward()\n",
        "\n",
        "        # Extract gradients\n",
        "        dw = w.grad\n",
        "        db = b.grad\n",
        "\n",
        "        # Update Parameters\n",
        "        # Note, we are doing manual updates, so we don't want these gradients tracked farther (because we are manually tracking them for demonstration purposes)xw\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * dw\n",
        "            b -= learning_rate * db\n",
        "\n",
        "    # Detach is special to torch. w contains more than just the values in the vector\n",
        "    # w is also part of a (potentially large) computation graph,\n",
        "    # detaching variables when you don't need the computation graph anymore saves\n",
        "    # significant amounts of RAM.\n",
        "    model_params = {\"w\": w.detach().numpy(), \"b\": b.detach().numpy()}\n",
        "\n",
        "    return model_params, params_list\n",
        "\n",
        "def predict_torch(w, b, X):\n",
        "    \"\"\"\n",
        "    Prediction method for torch. Run the forward method and turn p_pred into 0 or 1 labels.\n",
        "\n",
        "    Inputs:\n",
        "        w: Torch tensor of weights of model\n",
        "    Outputs:\n",
        "        predictions: Predicted labels of X, either 0 or 1\n",
        "    \"\"\"\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    p_pred = forward_torch(w, b, X)\n",
        "    predictions = (p_pred > 0.5).float()\n",
        "    return predictions\n",
        "\n",
        "# Example training with PyTorch\n",
        "n_features = X.shape[1]\n",
        "w, b = initialize_parameters(n_features)\n",
        "params_torch, params_list = optimize_pytorch(w, b, X, y, num_iterations=500, learning_rate=0.001, track_gradients=True)\n",
        "\n",
        "# Predictions and accuracy\n",
        "predictions_torch = predict_torch(w, b, X)\n",
        "accuracy_torch = torch.mean((predictions_torch == torch.tensor(y)).float())\n",
        "print(f\"PyTorch model accuracy: {accuracy_torch:.2f}%\")\n",
        "\n",
        "plot_decision_boundary_torch(params_torch[\"w\"], params_torch[\"b\"], X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aicPOYsh2WeD"
      },
      "source": [
        "## Part 3: Build and Train a Simple Perceptron with PyTorch\n",
        "\n",
        "In this part of the assignment, we will introduce you to torch's neural network module (called `nn`). This time, we use torch to build a simple perceptron (i.e., a neural network with no hidden layers) with a sigmoid activation function, so that this neural network encodes logistic regression. Once again, this code should function more or less as our implementation of logistic regression in torch, and as your implementation of logistic regression from scratch.\n",
        "\n",
        "You should use the following functions provided by `nn`.\n",
        "`nn.Linear`, which creates a linear layer (the most common type of layer in neural networks).\n",
        "`nn.Sigmoid`, which creates a sigmoid activation function.\n",
        "\n",
        "You should create these in `__init__`, and use them for the forward pass of the model.\n",
        "\n",
        "You can find the torch documentation [here](https://pytorch.org/docs/stable/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmZw0QnV2WeD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SingleLayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple single neuron (perceptron) torch model.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim: number of input features d\n",
        "            output_dim: number of output features (1, in this case)\n",
        "        \"\"\"\n",
        "        super(SingleLayerPerceptron, self).__init__()\n",
        "        # TODO\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        The forward method (i.e., go from inputs (X) to outputs)\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        return self.activation(self.linear)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaIQ6g2x2WeD"
      },
      "source": [
        "But wait... there's a forward method for our model, don't we have to write the backward pass as well? No! Since `SingleLayerPerceptron` inherits from `nn.Module`, it is provided with a `backward` method already that automatically computes the gradients of the model's parameters! That's the beauty of working with PyTorch!\n",
        "\n",
        "There's one other important note about `nn.Module`: `nn.Module` overloads the parentheses operator (`__call__`) to run the forward method. To run the forward pass on a model, you can either call `model.forward(X)` or `model(X)`, they are equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwy-mFHQ2WeD"
      },
      "outputs": [],
      "source": [
        "# Compute loss\n",
        "def loss_mlp(p_pred, y):\n",
        "    \"\"\"\n",
        "    Compute Log Loss (BCE)\n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "    loss = -(1 / m) * torch.sum(y * torch.log(p_pred) + (1 - y) * torch.log(1 - p_pred))\n",
        "    return loss\n",
        "\n",
        "# Train the MLP\n",
        "def train_mlp(model, X, y, num_iterations, learning_rate):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Forward propagation\n",
        "        p_pred = model.forward(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_mlp(p_pred, y)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters manually\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                param -= learning_rate * param.grad\n",
        "\n",
        "        # Zero the gradients\n",
        "        # (if you don't clear the gradients, the parameters will continue to accumulate information)\n",
        "        model.zero_grad()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Predict function for MLP\n",
        "def predict_mlp(model, X):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        output = model.forward(X)\n",
        "        predictions = (output > 0.5).float()\n",
        "    return predictions\n",
        "\n",
        "def accuracy_torch(model, X, y):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y)\n",
        "    with torch.no_grad():\n",
        "        p_pred = model.forward(X)\n",
        "        predictions = (p_pred > 0.5).float()\n",
        "        return torch.mean((predictions == y).float())\n",
        "\n",
        "# Visualize decision boundary for MLP\n",
        "def plot_decision_boundary_mlp(model, X, y):\n",
        "    X = X.numpy()\n",
        "    y = y.numpy()\n",
        "    x_min, x_max = X[:, 0].min() - 0.25, X[:, 0].max() + 0.25\n",
        "    y_min, y_max = X[:, 1].min() - 0.25, X[:, 1].max() + 0.25\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "    X_new = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    Z = model(X_new)\n",
        "    Z = (Z > 0.5).float().detach().numpy().reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=1, levels=[-1, 0, 1], colors=[\"#E4C483\", \"#C3FFF6\"])\n",
        "    color_dict = {0 : BLUE_PIXEL / 256, 1 : BROWN_PIXEL / 256}\n",
        "    colors = [color_dict[label] for label in y[:, 0]]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=5)\n",
        "    plt.title(\"Decision Boundary\")\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSo-BzaH2WeD"
      },
      "outputs": [],
      "source": [
        "def train_mlp(model, X, y, num_iterations, learning_rate):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    for i in range(num_iterations+1):\n",
        "        # Forward propagation\n",
        "        p_pred = model.forward(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_mlp(p_pred, y)\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters manually\n",
        "        with torch.no_grad():\n",
        "            # Loop over every parameter, update that parameter\n",
        "            for param in model.parameters():\n",
        "                param -= learning_rate * param.grad\n",
        "\n",
        "        # Zero the gradients\n",
        "        # (if you don't clear the gradients, the parameters will continue to accumulate information)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Record the loss\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHOdo9tF2WeD"
      },
      "outputs": [],
      "source": [
        "n_features = X.shape[1]\n",
        "model = SingleLayerPerceptron(input_dim=n_features, output_dim=1)\n",
        "model = train_mlp(model, X, y, 1000, 0.1)\n",
        "\n",
        "plot_decision_boundary_mlp(model, torch.tensor(X), torch.tensor(y))\n",
        "model_accuracy = predict_mlp(model, X)\n",
        "print(\"Final Model Accuracy: \", accuracy_torch(model, X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6q6sMWlz9l"
      },
      "source": [
        "## Part 4: Build and Train a Multilayer Perceptron with PyTorch\n",
        "\n",
        "Now it's your turn to come up with your own Neural Network! Add Torch operations to MyMLP below. Here are some of the parameters you might want to play around with:\n",
        "\n",
        "Number and size of layers in your Neural Network:\n",
        "\n",
        "For now, you should use `nn.Linear`, which is the standard (feed-foward) layer for neural networks.\n",
        "`nn.Linear` takes two parameters, the input dimension and the output dimension for the layer. They key thing to know when constructing your network, is that the size of the output of a linear layer must match the input size of the following layer. If they do not match, you'll get a shape error.\n",
        "\n",
        "After every linear layer, you should add an activation function.\n",
        "\n",
        "**Conceptual checkpoint:** Why is it important to add a non-linear activation function to each layer?\n",
        "\n",
        "Activation Functions:\n",
        "1. `nn.ReLU`: Rectified Linear Unit. ReLU(x) = x, if $x\\geq 0$, else ReLU(x)$ = 0$.\n",
        "1. `nn.Tanh`: Hyperbolic Tan Function.\n",
        "1. `nn.Sigmoid`: Can be used as an activation function in hidden layers, as well as for the output neuron.\n",
        "\n",
        "Hint: In our experience, Tanh and Sigmoids work best for this classification problem.\n",
        "\n",
        "You may also want to experiment with the number of iterations gradient descent is run for and the learning rate of your model as well.\n",
        "\n",
        "### Task: MyMLP\n",
        "Find a set of parameters for you `MyMLP` that separates the data much better than your logistic regressions. For full credit, you should achieve greater than 90% accuracy with your model. We have provided a working (but perhaps overly complicated) model for you below this section called `StaffMLP`. This may help you to see a working Torch Neural Network class. Obviously, your model should be different from `StaffMLP`.\n",
        "\n",
        "It should satisfy the following requirements:\n",
        "1. Consist of multiple hidden layers (layers that are not the input or output of the network)\n",
        "1. These hidden layers should not all be the same size, their input and output dimensions should differ in some way.\n",
        "\n",
        "After you complete this task, you should answer the following questions in your README:\n",
        "1. What hyper-parameters did you experiment with (i.e., number and size of layers, activation functions, learning rates, etc.)\n",
        "1. What effect on performance did these hyper-parameters have? How did you happen to your final setting, was it random search or did you use some other reasoning?\n",
        "1. What effect would you expect doubling the size of each layer to have (other than the input and output layers)? Try your model with more parameters, does it perform better/worse/the same?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC4yYyBR2WeD"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim: Number of features. Must match input dimension of first linear layer\n",
        "            output_dim: Number of outputs (1 in this case). Must match the output dimension of your final linear layer\n",
        "        \"\"\"\n",
        "        super(MyMLP, self).__init__()\n",
        "        # TODO: Add model layers and activation functions\n",
        "        self.layer1 = nn.Linear(input_dim, 16)\n",
        "        self.layer2 = nn.Linear(16,32)\n",
        "        self.layer3 = nn.Linear(32,16)\n",
        "        self.output_layer = nn.Linear(16, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        The forward pass of your neural network.\n",
        "        Take in input X and run each layer of the network.\n",
        "        Your forward pass should use each of the layers in your network\n",
        "        \"\"\"\n",
        "        # TODO: Add forward pass\n",
        "        X = self.relu(self.layer1(X))\n",
        "        X = self.relu(self.layer2(X))\n",
        "        X = self.relu(self.layer3(X))\n",
        "        X = self.sigmoid(self.output_layer(X))\n",
        "        return X\n",
        "\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "output_dim = 1 # The output dimension should be 1\n",
        "model = MyMLP(input_dim, output_dim)\n",
        "model = train_mlp(model, X, y, 50000, learning_rate=0.05)\n",
        "\n",
        "# Final predictions and accuracy\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "predictions_mlp = predict_mlp(model, X_tensor)\n",
        "accuracy_mlp = torch.mean((predictions_mlp == torch.tensor(y)).float())\n",
        "print(f\"Final MLP model accuracy: {accuracy_mlp:.2f}%\")\n",
        "plot_decision_boundary_mlp(model, torch.tensor(X), torch.tensor(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPAu3d392WeE"
      },
      "source": [
        "## Part 5: Our model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PnIomkOl2ou"
      },
      "outputs": [],
      "source": [
        "# Define the MLP class\n",
        "class StaffMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(StaffMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, output_dim) # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        z1 = self.fc1(X)\n",
        "        a1 = torch.relu(z1)  # Activation function for hidden layer\n",
        "        z2 = self.fc2(a1)\n",
        "        a2 = self.tanh(z2)\n",
        "        z3 = self.fc3(a2)\n",
        "        a3 = torch.relu(z3)\n",
        "        z4 = self.fc4(a3)\n",
        "        a4 = self.tanh(z4)\n",
        "        z5 = self.fc5(a4)\n",
        "        output = self.sigmoid(z5)  # Sigmoid activation for output\n",
        "        return output\n",
        "\n",
        "# Example training with the MLP\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 32  # You can change the number of hidden units\n",
        "output_dim = 1 # The output dimension should be 1\n",
        "model = StaffMLP(input_dim, hidden_dim, output_dim)\n",
        "model = train_mlp(model, X, y, 50000, learning_rate=0.001)\n",
        "\n",
        "# Final predictions and accuracy\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "predictions_mlp = predict_mlp(model, X_tensor)\n",
        "accuracy_mlp = torch.mean((predictions_mlp == torch.tensor(y)).float())\n",
        "print(f\"Final MLP model accuracy: {accuracy_mlp:.2f}%\")\n",
        "plot_decision_boundary_mlp(model, torch.tensor(X), torch.tensor(y))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}